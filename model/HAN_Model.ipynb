{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RNXEhSEVRGPc",
    "outputId": "1f383afa-a26b-4ef2-80df-d4673d06ed5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kss\n",
      "  Downloading kss-6.0.4.tar.gz (1.1 MB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.1 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting emoji==1.2.0 (from kss)\n",
      "  Downloading emoji-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pecab (from kss)\n",
      "  Downloading pecab-1.0.8.tar.gz (26.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from kss) (3.4.2)\n",
      "Collecting jamo (from kss)\n",
      "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting hangul-jamo (from kss)\n",
      "  Downloading hangul_jamo-1.0.1-py3-none-any.whl.metadata (899 bytes)\n",
      "Collecting tossi (from kss)\n",
      "  Downloading tossi-0.3.1.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting distance (from kss)\n",
      "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pyyaml==6.0 (from kss)\n",
      "  Downloading PyYAML-6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting unidecode (from kss)\n",
      "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting cmudict (from kss)\n",
      "  Downloading cmudict-1.0.32-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting koparadigm (from kss)\n",
      "  Downloading koparadigm-0.10.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting kollocate (from kss)\n",
      "  Downloading kollocate-0.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting bs4 (from kss)\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from kss) (2.0.2)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from kss) (8.3.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from kss) (1.15.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4->kss) (4.13.4)\n",
      "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->kss) (8.7.0)\n",
      "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->kss) (6.5.2)\n",
      "Collecting whoosh (from kollocate->kss)\n",
      "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting xlrd==1.2.0 (from koparadigm->kss)\n",
      "  Downloading xlrd-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from pecab->kss) (18.1.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from pecab->kss) (2024.11.6)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->kss) (2.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest->kss) (24.2)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->kss) (1.5.0)\n",
      "Collecting bidict (from tossi->kss)\n",
      "  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from tossi->kss) (1.17.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5->cmudict->kss) (3.21.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4->kss) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4->kss) (4.13.2)\n",
      "Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.9/757.9 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading cmudict-1.0.32-py3-none-any.whl (939 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hangul_jamo-1.0.1-py3-none-any.whl (4.4 kB)\n",
      "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
      "Downloading kollocate-0.0.2-py3-none-any.whl (72.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading koparadigm-0.10.0-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
      "Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: kss, distance, pecab, tossi\n",
      "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kss: filename=kss-6.0.4-cp311-cp311-linux_x86_64.whl size=1452482 sha256=e1f0a9ee9e79a746dcbe7cc2e8cfd9b85f0dab9965b3dfed1992679105fe3df6\n",
      "  Stored in directory: /root/.cache/pip/wheels/86/c9/5f/69b8fe9751eefbb5d087932ddf58d0f121b8e545335af7fe4e\n",
      "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16256 sha256=5950056eff50b5d4099923e1a1b6dd0c357deaf713d14789a3e2ddf1e26572c7\n",
      "  Stored in directory: /root/.cache/pip/wheels/fb/cd/9c/3ab5d666e3bcacc58900b10959edd3816cc9557c7337986322\n",
      "  Building wheel for pecab (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pecab: filename=pecab-1.0.8-py3-none-any.whl size=26646664 sha256=2866a83af6841d212d65e53bb46eec33f48fc585de3c12a6cbf4963c01ce53e6\n",
      "  Stored in directory: /root/.cache/pip/wheels/c9/0d/97/ca2bb361e44a80f4c63efe6f6438ff903fd1ab5640eedabc1b\n",
      "  Building wheel for tossi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tossi: filename=tossi-0.3.1-py3-none-any.whl size=12129 sha256=d3c1a5336ff23fa734f10276b07212f3c110ece77cee893d009b61d423ec4746\n",
      "  Stored in directory: /root/.cache/pip/wheels/36/1a/7e/0b78039c20678a6682f03cca4295efaa5fb55a3d10d7e9837a\n",
      "Successfully built kss distance pecab tossi\n",
      "Installing collected packages: whoosh, jamo, hangul-jamo, emoji, distance, xlrd, unidecode, pyyaml, kollocate, bidict, tossi, pecab, koparadigm, cmudict, bs4, kss\n",
      "  Attempting uninstall: xlrd\n",
      "    Found existing installation: xlrd 2.0.1\n",
      "    Uninstalling xlrd-2.0.1:\n",
      "      Successfully uninstalled xlrd-2.0.1\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.2\n",
      "    Uninstalling PyYAML-6.0.2:\n",
      "      Successfully uninstalled PyYAML-6.0.2\n",
      "Successfully installed bidict-0.23.1 bs4-0.0.2 cmudict-1.0.32 distance-0.1.3 emoji-1.2.0 hangul-jamo-1.0.1 jamo-0.4.1 kollocate-0.0.2 koparadigm-0.10.0 kss-6.0.4 pecab-1.0.8 pyyaml-6.0 tossi-0.3.1 unidecode-1.4.0 whoosh-2.7.4 xlrd-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install kss\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lP_3ATg9WIaE"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_path = '/content/dataset.zip'\n",
    "extract_path = '/content/dataset'\n",
    "\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DbSnYFkzSlHa"
   },
   "outputs": [],
   "source": [
    "# ===== 데이터 로드 =====\n",
    "base_dir = '/content/dataset/dataset'\n",
    "\n",
    "def load_json_folder(folder):\n",
    "    data = []\n",
    "    for fname in os.listdir(folder):\n",
    "        if fname.endswith('.json'):\n",
    "            with open(os.path.join(folder, fname), encoding='utf-8') as f:\n",
    "                item = json.load(f)\n",
    "                if isinstance(item, dict):\n",
    "                    data.append(item['sourceDataInfo'])\n",
    "                elif isinstance(item, list):\n",
    "                    data.extend([d['sourceDataInfo'] for d in item])\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "train_df = load_json_folder(os.path.join(base_dir, 'Training'))\n",
    "valid_df = load_json_folder(os.path.join(base_dir, 'Validation'))\n",
    "test_df  = load_json_folder(os.path.join(base_dir, 'Test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Whe8oamMSozc",
    "outputId": "14f1ae06-826d-45fd-8be0-835ccec5451b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Oh! You have mecab in your environment. Kss will take this as a backend! :D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== HAN 전처리 =====\n",
    "SENT_MAXLEN = 16\n",
    "WORD_MAXLEN = 64\n",
    "\n",
    "# vocab 사전 구축\n",
    "all_texts = pd.concat([\n",
    "    train_df['newsTitle'] + '. ' + train_df['newsContent'],\n",
    "    valid_df['newsTitle'] + '. ' + valid_df['newsContent']\n",
    "])\n",
    "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "for doc in all_texts:\n",
    "    for sent in kss.split_sentences(str(doc)):\n",
    "        for word in sent.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "def encode_korean(text):\n",
    "    sents = kss.split_sentences(str(text))[:SENT_MAXLEN]\n",
    "    doc_idx = []\n",
    "    for sent in sents:\n",
    "        word_idx = [vocab.get(w, 1) for w in str(sent).split()[:WORD_MAXLEN]]\n",
    "        word_idx += [0] * (WORD_MAXLEN - len(word_idx))\n",
    "        doc_idx.append(word_idx)\n",
    "    while len(doc_idx) < SENT_MAXLEN:\n",
    "        doc_idx.append([0]*WORD_MAXLEN)\n",
    "    return torch.tensor(doc_idx, dtype=torch.long)\n",
    "\n",
    "class HANDNewsDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = (df['newsTitle'] + '. ' + df['newsContent']).tolist()\n",
    "        if 'useType' in df.columns:\n",
    "            self.labels = df['useType'].tolist()\n",
    "        else:\n",
    "            raise KeyError(\"'useType' 컬럼이 데이터프레임에 없습니다.\")\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        doc_tensor = encode_korean(self.texts[idx])\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return doc_tensor, label\n",
    "\n",
    "trainset = HANDNewsDataset(train_df)\n",
    "validset = HANDNewsDataset(valid_df)\n",
    "testset  = HANDNewsDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QbR5XSiTSrJF"
   },
   "outputs": [],
   "source": [
    "# ===== HAN 모델 정의 =====\n",
    "class WordAttention(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        self.context = nn.Parameter(torch.randn(hidden_size * 2))\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        u = torch.tanh(self.fc(out))\n",
    "        attn = torch.matmul(u, self.context)\n",
    "        attn = F.softmax(attn, dim=1).unsqueeze(-1)\n",
    "        s = torch.sum(out * attn, dim=1)\n",
    "        return s\n",
    "\n",
    "class SentenceAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        self.context = nn.Parameter(torch.randn(hidden_size * 2))\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        u = torch.tanh(self.fc(out))\n",
    "        attn = torch.matmul(u, self.context)\n",
    "        attn = F.softmax(attn, dim=1).unsqueeze(-1)\n",
    "        v = torch.sum(out * attn, dim=1)\n",
    "        return v\n",
    "\n",
    "class HAN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=128, hidden_size=64, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.word_attn = WordAttention(embed_size, hidden_size)\n",
    "        self.sen_attn = SentenceAttention(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    def forward(self, x):  # x: (B, S, W)\n",
    "        B, S, W = x.shape\n",
    "        sents = []\n",
    "        for s in range(S):\n",
    "            e = self.embedding(x[:, s, :])\n",
    "            s_vec = self.word_attn(e)\n",
    "            sents.append(s_vec)\n",
    "        s_mat = torch.stack(sents, dim=1)\n",
    "        doc_vec = self.sen_attn(s_mat)\n",
    "        out = self.fc(doc_vec)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAtwj8WkStBZ"
   },
   "outputs": [],
   "source": [
    "# ===== 모델 초기화 및 학습 설정 =====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "han_model = HAN(vocab_size=VOCAB_SIZE)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    han_model = nn.DataParallel(han_model)\n",
    "han_model.to(device)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(validset, batch_size=32)\n",
    "test_loader = DataLoader(testset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3nJpt10SvBU"
   },
   "outputs": [],
   "source": [
    "# ===== 학습 함수 =====\n",
    "def train(model, loader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for batch in loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits = model(inputs)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkQVBGMsSz0K"
   },
   "outputs": [],
   "source": [
    "# ===== 평가 함수 =====\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logits = model(inputs)\n",
    "            preds = logits.argmax(dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(classification_report(all_labels, all_preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEQWJE_MT4at",
    "outputId": "19756b79-98a2-4133-dfb9-4f9655e1b195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8223    0.7855    0.8035      4000\n",
      "           1     0.7947    0.8303    0.8121      4000\n",
      "\n",
      "    accuracy                         0.8079      8000\n",
      "   macro avg     0.8085    0.8079    0.8078      8000\n",
      "weighted avg     0.8085    0.8079    0.8078      8000\n",
      "\n",
      "Epoch 2 completed.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8710    0.8085    0.8386      4000\n",
      "           1     0.8213    0.8802    0.8498      4000\n",
      "\n",
      "    accuracy                         0.8444      8000\n",
      "   macro avg     0.8462    0.8444    0.8442      8000\n",
      "weighted avg     0.8462    0.8444    0.8442      8000\n",
      "\n",
      "Epoch 3 completed.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8995    0.8455    0.8716      4000\n",
      "           1     0.8542    0.9055    0.8791      4000\n",
      "\n",
      "    accuracy                         0.8755      8000\n",
      "   macro avg     0.8769    0.8755    0.8754      8000\n",
      "weighted avg     0.8769    0.8755    0.8754      8000\n",
      "\n",
      "Epoch 4 completed.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9126    0.9058    0.9092      4000\n",
      "           1     0.9065    0.9133    0.9098      4000\n",
      "\n",
      "    accuracy                         0.9095      8000\n",
      "   macro avg     0.9095    0.9095    0.9095      8000\n",
      "weighted avg     0.9095    0.9095    0.9095      8000\n",
      "\n",
      "Epoch 5 completed.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9292    0.9190    0.9241      4000\n",
      "           1     0.9199    0.9300    0.9249      4000\n",
      "\n",
      "    accuracy                         0.9245      8000\n",
      "   macro avg     0.9246    0.9245    0.9245      8000\n",
      "weighted avg     0.9246    0.9245    0.9245      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== 학습 루프 =====\n",
    "optimizer = torch.optim.Adam(han_model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train(han_model, train_loader, optimizer, device, epoch)\n",
    "    evaluate(han_model, valid_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFzfzb3ET61q"
   },
   "outputs": [],
   "source": [
    "# ===== 예측 함수 (뉴스 제목 + 본문 입력) =====\n",
    "def predict_news(title, content):\n",
    "    han_model.eval()\n",
    "    text = f\"{title}. {content}\"\n",
    "    with torch.no_grad():\n",
    "        encoded = encode_korean(text).unsqueeze(0).to(device)  # (1, S, W)\n",
    "        output = han_model(encoded)\n",
    "        pred = output.argmax(dim=-1).item()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BASjU7sA6eft"
   },
   "outputs": [],
   "source": [
    "# ===== 모델 저장 =====\n",
    "import pickle\n",
    "\n",
    "# 저장할 정보를 딕셔너리로 구성\n",
    "save_data = {\n",
    "    'model_state': han_model.module.state_dict() if hasattr(han_model, 'module') else han_model.state_dict(),\n",
    "    'vocab': vocab,\n",
    "    'config': {\n",
    "        'SENT_MAXLEN': SENT_MAXLEN,\n",
    "        'WORD_MAXLEN': WORD_MAXLEN,\n",
    "        'num_classes': 2,\n",
    "        'embed_size': 128,\n",
    "        'hidden_size': 64\n",
    "    }\n",
    "}\n",
    "\n",
    "# 파일 저장\n",
    "with open('han_model.pkl', 'wb') as f:\n",
    "    pickle.dump(save_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CPU 모드로 모델 저장 =====\n",
    "def save_model_cpu_mode(model, vocab, filename='han_model_cpu.pkl'):\n",
    "    # 모델 설정 파라미터 추출\n",
    "    config = {\n",
    "        'SENT_MAXLEN': SENT_MAXLEN,\n",
    "        'WORD_MAXLEN': WORD_MAXLEN,\n",
    "        'num_classes': 2,\n",
    "        'embed_size': 128,\n",
    "        'hidden_size': 64\n",
    "    }\n",
    "    \n",
    "    # 모델 상태를 CPU로 이동\n",
    "    model_state_cpu = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n",
    "    for key in model_state_cpu:\n",
    "        model_state_cpu[key] = model_state_cpu[key].cpu()\n",
    "\n",
    "    save_data = {\n",
    "        'model_state': model_state_cpu,\n",
    "        'vocab': vocab,\n",
    "        'config': config\n",
    "    }\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(save_data, f)\n",
    "\n",
    "# 실제 저장 실행 (config 파라미터 제거)\n",
    "save_model_cpu_mode(han_model, vocab)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}